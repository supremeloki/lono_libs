# Lono Library

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A lightweight and modular Python library for comprehensive machine learning model evaluation and comparison.

## üöÄ Overview

`lono_libs` provides a streamlined framework to evaluate various classification and regression models across multiple metrics, allowing for custom metric weighting, performance reporting, and visualization. It emphasizes modularity, extensibility, and ease of use, making it ideal for MLOps workflows and research.

## ‚ú® Features

- **Modular Metric Calculation:** Easily add and manage custom evaluation metrics for both classification and regression
- **Flexible Model Evaluation:** Evaluate a wide range of models with configurable metrics and data splits
- **Weighted Scoring:** Define custom weights for metrics to generate a composite score for model ranking
- **Automated Reporting & Visualization:** Generate performance reports and insightful plots automatically
- **Dependency Injection:** A robust architecture ensuring low coupling and high maintainability

## üöß Structure

```
lono_libs/
‚îú‚îÄ‚îÄ .flake8
‚îú‚îÄ‚îÄ .gitattributes
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ run_all_tests.py
‚îú‚îÄ‚îÄ run_evaluation.py
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ api.rst
‚îÇ   ‚îú‚îÄ‚îÄ conf.py
‚îÇ   ‚îú‚îÄ‚îÄ examples.rst
‚îÇ   ‚îú‚îÄ‚îÄ index.rst
‚îÇ   ‚îú‚îÄ‚îÄ make.bat
‚îÇ   ‚îú‚îÄ‚îÄ Makefile
‚îÇ   ‚îú‚îÄ‚îÄ metrics.rst
‚îÇ   ‚îî‚îÄ‚îÄ quickstart_usage.rst
‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ example_correlation.py
‚îÇ   ‚îú‚îÄ‚îÄ example_multiple_metrics.py
‚îÇ   ‚îî‚îÄ‚îÄ quick start.py
‚îú‚îÄ‚îÄ lono_libs/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ best_result.py
‚îÇ   ‚îú‚îÄ‚îÄ data_prep.py
‚îÇ   ‚îú‚îÄ‚îÄ models.py
‚îÇ   ‚îú‚îÄ‚îÄ reporting.py
‚îÇ   ‚îú‚îÄ‚îÄ summary_reports.py
‚îÇ   ‚îú‚îÄ‚îÄ unified_runner.py
‚îÇ   ‚îú‚îÄ‚îÄ visualization.py
‚îÇ   ‚îú‚îÄ‚îÄ classification/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ accuracy.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ balanced_accuracy.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cohens_kappa.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ confusion_matrix.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ f1_score.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LogLoss.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ matthews_correlation_coefficient.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ precision.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ recall.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ roc_auc.py
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ _base_metric.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluator.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scoring.py
‚îÇ   ‚îî‚îÄ‚îÄ regression/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ correlation.py
‚îÇ       ‚îú‚îÄ‚îÄ kurtosis.py
‚îÇ       ‚îú‚îÄ‚îÄ mae.py
‚îÇ       ‚îú‚îÄ‚îÄ mse.py
‚îÇ       ‚îú‚îÄ‚îÄ r2_score.py
‚îÇ       ‚îî‚îÄ‚îÄ skewness.py
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ Accuracy_mock.py
    ‚îú‚îÄ‚îÄ IMetric_mock.py
    ‚îú‚îÄ‚îÄ UnifiedRunner_mock.py
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ test_evaluator.py
    ‚îú‚îÄ‚îÄ test_unified_runner.py
    ‚îú‚îÄ‚îÄ classification/
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_accuracy.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_balanced_accuracy.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_cohens_kappa.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_confusion_matrix.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_f1_score.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_log_loss.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_matthews_correlation_coefficient.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_precision.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_recall.py
    ‚îÇ   ‚îî‚îÄ‚îÄ test_roc_auc.py
    ‚îî‚îÄ‚îÄ regression/
        ‚îú‚îÄ‚îÄ __init__.py
        ‚îú‚îÄ‚îÄ test_correlation.py
        ‚îú‚îÄ‚îÄ test_kurtosis.py
        ‚îú‚îÄ‚îÄ test_mae.py
        ‚îú‚îÄ‚îÄ test_mse.py
        ‚îú‚îÄ‚îÄ test_r2_score.py
        ‚îî‚îÄ‚îÄ test_skewness.py
```
## üì¶ Installation

### From PyPI
```bash
pip install lono_libs
```

### For Development
```bash
# Clone the repository
git clone https://github.com/supremeloki/lono_libs.git
cd lono_libs

# Create and activate virtual environment
python -m venv .venv
# On Windows
.venv\Scripts\activate
# On Unix/Mac
source .venv/bin/activate

# Install with development dependencies
pip install -e ".[dev,test,docs]"
```

## üìã Requirements

- Python 3.9 or higher
- Required dependencies:
  - numpy >= 1.20
  - pandas >= 1.3
  - scikit-learn >= 1.0
  - matplotlib >= 3.4
  - scipy >= 1.7
  - xgboost >= 1.6
  - lightgbm >= 3.3
  - catboost >= 1.0

## üèÉ Quick Start

Here's a simple example of how to use `lono_libs`:

```python
from lono_libs import Evaluator
from lono_libs.classification import accuracy, f1_score
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# Create sample data
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a model
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Create evaluator with metrics
evaluator = Evaluator(metrics=[accuracy, f1_score])

# Evaluate model
results = evaluator.evaluate(model, X_test, y_test)
print(results.summary())
```

## üìä Available Metrics

### Classification Metrics
- Accuracy
- Precision
- Recall
- F1 Score
- Balanced Accuracy
- Cohen's Kappa
- Matthews Correlation Coefficient
- ROC AUC
- Log Loss
- Confusion Matrix

### Regression Metrics
- Mean Absolute Error (MAE)
- Mean Squared Error (MSE)
- R¬≤ Score
- Correlation
- Kurtosis
- Skewness

## ‚ö° Performance Benchmarks

### Evaluation Speed
- **10,000 samples, 50 features**: ~0.15-0.25 seconds for 4 metrics
- **Throughput**: ~40,000-65,000 samples/second
- **Memory Usage**: Scales linearly with dataset size

### Benchmark Example
```python
import time
from lono_libs import Evaluator
from lono_libs.classification import accuracy, f1_score, precision, recall

# 10K samples, 50 features dataset
evaluator = Evaluator(metrics=[accuracy, f1_score, precision, recall])

start_time = time.time()
results = evaluator.evaluate(model, X_test, y_test)
eval_time = time.time() - start_time

print(f"Evaluation completed in {eval_time:.4f} seconds")
# Output: Evaluation completed in 0.1876 seconds
```

### System Requirements
- **Minimum**: Python 3.9+, 4GB RAM, modern CPU
- **Recommended**: Python 3.9+, 8GB+ RAM, multi-core CPU
- **Optimal**: Python 3.11+, 16GB+ RAM, high-core CPU for parallel processing
## üîß Usage with UnifiedRunner

For comprehensive model evaluation across multiple algorithms:

```python
from lono_libs import UnifiedRunner

runner = UnifiedRunner(
    output_base_dir="results",
    enable_logging=True,
    enable_visualizations=True
)

# Run evaluation pipeline
results_df, best_models = runner.run_pipeline(
    X_train=X_train,
    X_test=X_test,
    y_train_class=y_train_class,
    y_test_class=y_test_class,
    y_train_reg=None,
    y_test_reg=None,
    preprocessing_config={
        "add_polynomial_features": True,
        "poly_degree": 2,
        "apply_scaler": True
    },
    metrics_to_evaluate=["Accuracy", "F1Score", "Precision", "Recall"]
)
```

## üß™ Testing

Run the full test suite:
```bash
python run_all_tests.py
```

Or use pytest directly:
```bash
pytest --cov=lono_libs tests/
```

## üìù Code Quality

We maintain high code quality standards:
- `black` for code formatting
- `ruff` for linting
- `mypy` for type checking

Run quality checks:
```bash
black .
ruff check .
mypy lono_libs
```

## üìö Documentation

Build documentation locally:
```bash
cd docs
make html
```

## ü§ù Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üë§ Author

**Kooroush Masoumi** - [kooroushmasoumi@gmail.com](mailto:kooroushmasoumi@gmail.com)

---

‚≠ê Star this repo if you find it useful!